{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Catalyst, Albumentations, Pytorch Toolbelt showcase: Semantic Segmentation @ CamVid",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zrghassabi/OCT-image-segmentation/blob/main/Catalyst%2C_Albumentations%2C_Pytorch_Toolbelt_showcase_Semantic_Segmentation_CamVid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtpTRdGLE9kc"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQWxqrZ6E_-5"
      },
      "source": [
        "This notebook is a quick-start guide into semantic segmentation using Catalyst deep learning library. It starts with baseline training pipeline and then goes into advanced details. Topics covered:\n",
        "\n",
        "\n",
        "*  Baseline training with reducing LR on plataeu\n",
        "*  Baseline training with classes balancing \n",
        "*  Visualization of batches during training using Catalyst callbacks\n",
        "*  Fine-tuning part of the segmentation model\n",
        "*  Fine-tuning model with different loss functions\n",
        "*  Fine-tuning model with multiple  LR\n",
        "\n",
        "In this tutorial we use reduced version of CamVid dataset from https://github.com/alexgkendall/SegNet-Tutorial.  \n",
        "\n",
        "Disclaimer: In this notebook we use replatively simple model, which does not reach SOTA results (which is about 0.81 according to https://paperswithcode.com/sota/semantic-segmentation-on-camvid). Yet it sufficient to demonstrate key concepts of Catalyst.\n",
        "\n",
        "## Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4_MwE7el_2Y",
        "outputId": "4548156e-f021-4dc0-cce6-dba128db6501",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Install required libs\n",
        "! pip install --quiet torch torchvision torchcontrib\n",
        "! pip install --quiet git+https://github.com/albu/albumentations\n",
        "! pip install --quiet catalyst==19.7.4 pytorch_toolbelt==0.1.3"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for torchcontrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 47.6 MB 1.7 MB/s \n",
            "\u001b[?25h  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 212 kB 5.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 59 kB 6.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 44.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 171 kB 45.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 124 kB 47.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 49 kB 4.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 29.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 676 kB 38.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.7 MB/s \n",
            "\u001b[?25h  Building wheel for pytorch-toolbelt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torchnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torchfile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray 0.18.2 requires pandas>=1.0, but you have pandas 0.24.2 which is incompatible.\n",
            "plotnine 0.6.0 requires pandas>=0.25.0, but you have pandas 0.24.2 which is incompatible.\n",
            "mizani 0.6.0 requires pandas>=0.25.0, but you have pandas 0.24.2 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 0.24.2 which is incompatible.\n",
            "fbprophet 0.7.1 requires pandas>=1.0.4, but you have pandas 0.24.2 which is incompatible.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ST9bjtjtBT2P"
      },
      "source": [
        "Catalyst supports logging to Tensorboard, so let's install Tensorboard support for notebooks to see the training progress. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6EGuAmTNG0V",
        "outputId": "d1f0e26e-f581-4f7d-c77c-61cc2d520e4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -q tf-nightly-2.0-preview\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tf-nightly-2.0-preview (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for tf-nightly-2.0-preview\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-fDLSPK3wBM"
      },
      "source": [
        "##  Mixed precision training support"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Wdvottj4vWN"
      },
      "source": [
        "The bigger our model are, the bigger spatial resolution of images during training, the more GPU memory we need. As memory footprint per image grows, batch size must be reduced in order to stay within available GPU memory limit. \n",
        "However, small batch size can make BatchNormalization unstable and make training converge slower. To address this, we can employ mixed precision training with is supported by Catalyst.\n",
        "\n",
        "Under the hood, Catalyst uses NVidia Apex library, which must be installed separately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf-4fhZsE5I5",
        "outputId": "32614302-825e-44a5-b62e-a5e17847692a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Install Apex for mixed-precision training\n",
        "! rm -rf apex\n",
        "! git clone https://github.com/NVIDIA/apex && cd apex && pip install --quiet -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ."
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 8815, done.\u001b[K\n",
            "remote: Counting objects: 100% (48/48), done.\u001b[K\n",
            "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
            "remote: Total 8815 (delta 20), reused 20 (delta 4), pack-reused 8767\u001b[K\n",
            "Receiving objects: 100% (8815/8815), 14.48 MiB | 4.35 MiB/s, done.\n",
            "Resolving deltas: 100% (6001/6001), done.\n",
            "/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py:232: UserWarning: Disabling all use of wheels due to the use of --build-option / --global-option / --install-option.\n",
            "  cmdoptions.check_install_build_global(options)\n",
            "Processing /content/apex\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Skipping wheel build for apex, due to binaries being disabled for it.\n",
            "Installing collected packages: apex\n",
            "  Attempting uninstall: apex\n",
            "    Found existing installation: apex 0.1\n",
            "    Uninstalling apex-0.1:\n",
            "      Successfully uninstalled apex-0.1\n",
            "    Running setup.py install for apex ... \u001b[?25l\u001b[?25hdone\n",
            "Successfully installed apex-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5mXLGHnj3GH"
      },
      "source": [
        "# Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97Bvr6bZE7Au"
      },
      "source": [
        "## Clone dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLPMkI6b2IuE"
      },
      "source": [
        "We start by cloning a dataset locally"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mc2u4SDzmHgF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b14e7d11-3aa1-495e-cfe5-527b72870a32"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "DATA_DIR = './data/CamVid/'\n",
        "\n",
        "# load repo with data if it is not exists\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    print('Loading data...')\n",
        "    os.system('git clone https://github.com/alexgkendall/SegNet-Tutorial ./data')\n",
        "    print('Done!')\n",
        "    \n",
        "x_train_dir = os.path.join(DATA_DIR, 'train')\n",
        "y_train_dir = os.path.join(DATA_DIR, 'trainannot')\n",
        "\n",
        "x_valid_dir = os.path.join(DATA_DIR, 'val')\n",
        "y_valid_dir = os.path.join(DATA_DIR, 'valannot')\n",
        "\n",
        "x_test_dir = os.path.join(DATA_DIR, 'test')\n",
        "y_test_dir = os.path.join(DATA_DIR, 'testannot')\n",
        "\n",
        "\n",
        "CLASS_NAMES = ['sky', 'building', 'pole', 'road', 'pavement',\n",
        "               'tree', 'signsymbol', 'fence', 'car',\n",
        "               'pedestrian', 'bicyclist', 'unlabelled']\n",
        "\n",
        "CLASS_COLORS = [(128, 128, 128), (128, 0, 0), (192, 192, 128), (128, 64, 128), (0, 0, 192),\n",
        "                (128, 128, 0), (192, 128, 128), (64, 64, 128), (64, 0, 128),\n",
        "                (64, 64, 0), (0, 128, 192), (0, 0, 0)]\n",
        "\n",
        "num_classes = len(CLASS_NAMES)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Llw2nyN8j5eq"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMBukGoxmBJJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "b21adf30-ac4b-4e9b-bc42-bd95ef9d0449"
      },
      "source": [
        "import os\n",
        "from collections import OrderedDict\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import albumentations as A\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import cv2\n",
        "from catalyst.contrib.schedulers import MultiStepLR, ReduceLROnPlateau\n",
        "from catalyst.dl import SupervisedRunner, EarlyStoppingCallback, SchedulerCallback\n",
        "from catalyst.utils import unpack_checkpoint, load_checkpoint\n",
        "from pytorch_toolbelt.inference.functional import pad_image_tensor, unpad_image_tensor\n",
        "from pytorch_toolbelt.optimization.functional import get_lr_decay_parameters\n",
        "from pytorch_toolbelt.utils import fs\n",
        "from pytorch_toolbelt.utils.catalyst import *\n",
        "from pytorch_toolbelt.utils.fs import id_from_fname\n",
        "from pytorch_toolbelt.utils.torch_utils import tensor_from_rgb_image\n",
        "from pytorch_toolbelt.utils.random import set_manual_seed\n",
        "from catalyst.utils import unpack_checkpoint, load_checkpoint\n",
        "from pytorch_toolbelt.losses import JointLoss, MulticlassDiceLoss, MulticlassJaccardLoss\n",
        "from torch.optim import Adam, SGD, ASGD, RMSprop, LBFGS\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torchvision.models import resnet34\n",
        "from datetime import datetime"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-69d872f9c7f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcatalyst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedulers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultiStepLR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcatalyst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSupervisedRunner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEarlyStoppingCallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSchedulerCallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcatalyst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0munpack_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'catalyst'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cz5CyFxwkXY2"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGeafxERkaL7"
      },
      "source": [
        "class CamVidDataset(Dataset):\n",
        "    \"\"\"CamVid Dataset. Read images, apply augmentation and preprocessing transformations.\n",
        "\n",
        "    Args:\n",
        "        images_dir (str): path to images folder\n",
        "        masks_dir (str): path to segmentation masks folder\n",
        "        class_values (list): values of classes to extract from segmentation mask\n",
        "        augmentation (A.Compose): data transfromation pipeline\n",
        "            (e.g. flip, scale, etc.)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            images_dir,\n",
        "            masks_dir,\n",
        "            transform=A.Normalize()\n",
        "    ):\n",
        "        self.ids = os.listdir(images_dir)\n",
        "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
        "        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # read data\n",
        "        image = fs.read_rgb_image(self.images_fps[i])\n",
        "        mask = fs.read_image_as_is(self.masks_fps[i])\n",
        "        assert mask.max() < len(CLASS_NAMES)\n",
        "\n",
        "        # apply augmentations\n",
        "        sample = self.transform(image=image, mask=mask)\n",
        "        image, mask = sample['image'], sample['mask']\n",
        "\n",
        "        return {\n",
        "            \"image_id\": id_from_fname(self.images_fps[i]),\n",
        "            \"features\": tensor_from_rgb_image(image),\n",
        "            \"targets\": torch.from_numpy(mask).long()\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIZc6OvOf52k"
      },
      "source": [
        "## Augmentations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EslTyvsKkOKz"
      },
      "source": [
        "def get_training_augmentation(blur=True, weather=True):\n",
        "    return A.Compose([\n",
        "        A.PadIfNeeded(384, 384, border_mode=cv2.BORDER_CONSTANT, value=0, mask_value=11),\n",
        "\n",
        "        A.OneOf([\n",
        "#             A.GridDistortion(border_mode=cv2.BORDER_CONSTANT, value=0, mask_value=11),\n",
        "#             A.ElasticTransform(alpha_affine=10, border_mode=cv2.BORDER_CONSTANT, value=0, mask_value=11),\n",
        "            A.ShiftScaleRotate(shift_limit=0, scale_limit=0, rotate_limit=10,\n",
        "                               border_mode=cv2.BORDER_CONSTANT, value=0, mask_value=11),\n",
        "            A.OpticalDistortion(border_mode=cv2.BORDER_CONSTANT, value=0, mask_value=11),\n",
        "            A.NoOp(p=0.6)\n",
        "        ]),\n",
        "\n",
        "        A.OneOf([\n",
        "            A.CLAHE(),\n",
        "            A.RandomBrightnessContrast(),\n",
        "            A.RandomGamma(),\n",
        "            A.HueSaturationValue(),\n",
        "            A.NoOp()\n",
        "        ]),\n",
        "\n",
        "        A.Compose([\n",
        "            A.OneOf([\n",
        "                A.IAASharpen(),\n",
        "                A.Blur(blur_limit=3),\n",
        "                A.MotionBlur(blur_limit=3),\n",
        "                A.ISONoise(),\n",
        "                A.NoOp()\n",
        "            ]),\n",
        "        ], p=float(blur)),\n",
        "\n",
        "        A.Compose([\n",
        "            A.OneOf([\n",
        "                A.RandomFog(),\n",
        "                A.RandomSunFlare(src_radius=100),\n",
        "                A.RandomRain(),\n",
        "                A.RandomSnow(),\n",
        "                A.NoOp()\n",
        "            ]),\n",
        "        ]) if weather else A.NoOp(),\n",
        "\n",
        "        A.RandomSizedCrop(min_max_height=(300, 360), height=320, width=320, always_apply=True),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.Cutout(),\n",
        "        A.Normalize(),\n",
        "    ])\n",
        "\n",
        "\n",
        "def get_validation_augmentation():\n",
        "    return A.Compose([\n",
        "        A.PadIfNeeded(384, 384, border_mode=cv2.BORDER_CONSTANT, value=0, mask_value=11),\n",
        "        A.Normalize()\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsVWsv2Sj7JH",
        "outputId": "61602150-0676-41d9-935a-3033ab79b897",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "train_ds = CamVidDataset(x_train_dir, y_train_dir, transform=get_training_augmentation())\n",
        "valid_ds = CamVidDataset(x_valid_dir, y_valid_dir, transform=get_validation_augmentation())\n",
        "\n",
        "print('Train dataset size', len(train_ds))\n",
        "print('Valid dataset size', len(valid_ds))\n",
        "\n",
        "# Define callbacks\n",
        "\n",
        "iou_score = JaccardScoreCallback(mode='multiclass',\n",
        "                             # We exclude last 'unlabeled' class from the evaluation\n",
        "                             class_names=CLASS_NAMES[:num_classes-1],\n",
        "                             classes_of_interest=np.arange(num_classes-1),\n",
        "                             prefix='iou')\n",
        "\n",
        "visualize_predictions = partial(draw_semantic_segmentation_predictions,\n",
        "                                mode='side-by-side',\n",
        "                                class_colors=CLASS_COLORS)\n",
        "\n",
        "show_batches = ShowPolarBatchesCallback(visualize_predictions,\n",
        "                                 targets=['tensorboard'],\n",
        "                                 metric='iou',\n",
        "                                 minimize=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train dataset size 367\n",
            "Valid dataset size 101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJyAJuzyEa2e"
      },
      "source": [
        "## Define a model\n",
        "\n",
        "For sake of demonstation purposes, we use a LinkNet model with Resnet34 encoder. Of course one can use more advanced model, however we still can get decent results with LinkNet34 in a short amount of time. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b6Qdtmh577s"
      },
      "source": [
        "## LinkNet34"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FZET6O26AVe"
      },
      "source": [
        "class DecoderBlockLinkNet(nn.Module):\n",
        "    def __init__(self, in_channels, n_filters):\n",
        "        super().__init__()\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # B, C, H, W -> B, C/4, H, W\n",
        "        self.conv1 = nn.Conv2d(in_channels, in_channels // 4, 1)\n",
        "        self.norm1 = nn.BatchNorm2d(in_channels // 4)\n",
        "\n",
        "        # B, C/4, H, W -> B, C/4, 2 * H, 2 * W\n",
        "        self.deconv2 = nn.ConvTranspose2d(in_channels // 4, in_channels // 4, kernel_size=4,\n",
        "                                          stride=2, padding=1, output_padding=0)\n",
        "        self.norm2 = nn.BatchNorm2d(in_channels // 4)\n",
        "\n",
        "        # B, C/4, H, W -> B, C, H, W\n",
        "        self.conv3 = nn.Conv2d(in_channels // 4, n_filters, 1)\n",
        "        self.norm3 = nn.BatchNorm2d(n_filters)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.norm1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.deconv2(x)\n",
        "        x = self.norm2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.norm3(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class LinkNet34(nn.Module):\n",
        "    def __init__(self, num_classes=1, num_channels=3, pretrained=True):\n",
        "        super().__init__()\n",
        "        assert num_channels == 3\n",
        "        self.num_classes = num_classes\n",
        "        filters = [64, 128, 256, 512]\n",
        "        resnet = resnet34(pretrained=pretrained)\n",
        "\n",
        "        self.firstconv = resnet.conv1\n",
        "        self.firstbn = resnet.bn1\n",
        "        self.firstrelu = resnet.relu\n",
        "        self.firstmaxpool = resnet.maxpool\n",
        "        self.encoder1 = resnet.layer1\n",
        "        self.encoder2 = resnet.layer2\n",
        "        self.encoder3 = resnet.layer3\n",
        "        self.encoder4 = resnet.layer4\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder4 = DecoderBlockLinkNet(filters[3], filters[2])\n",
        "        self.decoder3 = DecoderBlockLinkNet(filters[2], filters[1])\n",
        "        self.decoder2 = DecoderBlockLinkNet(filters[1], filters[0])\n",
        "        self.decoder1 = DecoderBlockLinkNet(filters[0], filters[0])\n",
        "\n",
        "        # Final Classifier\n",
        "        self.finaldeconv1 = nn.ConvTranspose2d(filters[0], 32, 3, stride=2)\n",
        "        self.finalrelu1 = nn.ReLU(inplace=True)\n",
        "        self.finalconv2 = nn.Conv2d(32, 32, 3)\n",
        "        self.finalrelu2 = nn.ReLU(inplace=True)\n",
        "        self.finalconv3 = nn.Conv2d(32, num_classes, 2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "      \n",
        "        # Encoder\n",
        "        x = self.firstconv(x)\n",
        "        x = self.firstbn(x)\n",
        "        x = self.firstrelu(x)\n",
        "        x = self.firstmaxpool(x)\n",
        "        e1 = self.encoder1(x)\n",
        "        e2 = self.encoder2(e1)\n",
        "        e3 = self.encoder3(e2)\n",
        "        e4 = self.encoder4(e3)\n",
        "\n",
        "        # Decoder with Skip Connections\n",
        "        d4 = self.decoder4(e4) + e3\n",
        "        d3 = self.decoder3(d4) + e2\n",
        "        d2 = self.decoder2(d3) + e1\n",
        "        d1 = self.decoder1(d2)\n",
        "\n",
        "        # Final Classification\n",
        "        f1 = self.finaldeconv1(d1)\n",
        "        f2 = self.finalrelu1(f1)\n",
        "        f3 = self.finalconv2(f2)\n",
        "        f4 = self.finalrelu2(f3)\n",
        "        f5 = self.finalconv3(f4)\n",
        "        \n",
        "        return f5 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mK0doTpYFUxj"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ifiDvvaNJEH"
      },
      "source": [
        "def restore_from_checkpoint(checkpoint_file, model, optimizer=None):\n",
        "  try:\n",
        "    checkpoint = load_checkpoint(checkpoint_file)\n",
        "  except FileNotFoundError:\n",
        "    print('Checkpoint not found', checkpoint_file)\n",
        "    return\n",
        "  \n",
        "  epoch = checkpoint['epoch']\n",
        "  valid_metrics = checkpoint['valid_metrics']\n",
        "\n",
        "  try:\n",
        "    unpack_checkpoint(checkpoint, model=model)     \n",
        "    print('Loaded model weights from epoch', epoch, 'Validation mIoU', valid_metrics['iou'])\n",
        "  except Exception as e:\n",
        "    print('Failed to restore model from checkpoint', checkpoint_file)\n",
        "    print(e)\n",
        "    \n",
        "  try:\n",
        "    if optimizer is not None:\n",
        "      unpack_checkpoint(checkpoint, optimizer=optimizer)\n",
        "  except Exception as e:\n",
        "    print('Failed to restore optimizer state from checkpoint', checkpoint_file)\n",
        "    print(e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KJfOuai3Y2a"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeE_zA_ARQT7",
        "outputId": "cb62c81d-f0f2-4bce-bdf0-c7d25a06401e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "os.makedirs('runs', exist_ok=True)\n",
        "%tensorboard --logdir runs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div id=\"root\"></div>\n",
              "    <script>\n",
              "      (function() {\n",
              "        window.TENSORBOARD_ENV = window.TENSORBOARD_ENV || {};\n",
              "        window.TENSORBOARD_ENV[\"IN_COLAB\"] = true;\n",
              "        document.querySelector(\"base\").href = \"https://localhost:6006\";\n",
              "        function fixUpTensorboard(root) {\n",
              "          const tftb = root.querySelector(\"tf-tensorboard\");\n",
              "          // Disable the fragment manipulation behavior in Colab. Not\n",
              "          // only is the behavior not useful (as the iframe's location\n",
              "          // is not visible to the user), it causes TensorBoard's usage\n",
              "          // of `window.replace` to navigate away from the page and to\n",
              "          // the `localhost:<port>` URL specified by the base URI, which\n",
              "          // in turn causes the frame to (likely) crash.\n",
              "          tftb.removeAttribute(\"use-hash\");\n",
              "        }\n",
              "        function executeAllScripts(root) {\n",
              "          // When `script` elements are inserted into the DOM by\n",
              "          // assigning to an element's `innerHTML`, the scripts are not\n",
              "          // executed. Thus, we manually re-insert these scripts so that\n",
              "          // TensorBoard can initialize itself.\n",
              "          for (const script of root.querySelectorAll(\"script\")) {\n",
              "            const newScript = document.createElement(\"script\");\n",
              "            newScript.type = script.type;\n",
              "            newScript.textContent = script.textContent;\n",
              "            root.appendChild(newScript);\n",
              "            script.remove();\n",
              "          }\n",
              "        }\n",
              "        function setHeight(root, height) {\n",
              "          // We set the height dynamically after the TensorBoard UI has\n",
              "          // been initialized. This avoids an intermediate state in\n",
              "          // which the container plus the UI become taller than the\n",
              "          // final width and cause the Colab output frame to be\n",
              "          // permanently resized, eventually leading to an empty\n",
              "          // vertical gap below the TensorBoard UI. It's not clear\n",
              "          // exactly what causes this problematic intermediate state,\n",
              "          // but setting the height late seems to fix it.\n",
              "          root.style.height = `${height}px`;\n",
              "        }\n",
              "        const root = document.getElementById(\"root\");\n",
              "        fetch(\".\")\n",
              "          .then((x) => x.text())\n",
              "          .then((html) => void (root.innerHTML = html))\n",
              "          .then(() => fixUpTensorboard(root))\n",
              "          .then(() => executeAllScripts(root))\n",
              "          .then(() => setHeight(root, 800));\n",
              "      })();\n",
              "    </script>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUWuiO5K3aUm"
      },
      "source": [
        "## Regular training\n",
        "\n",
        "We start with training a LinkNet model using pre-trained encoder with learning rate of 1e-3 for all layers, gradually lovering it with gamma=0.5 every 30 epochs. In total, we train network for 150 epochs using default CE loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b9UfZkNmYv_",
        "outputId": "ba28884f-c7e5-43d4-b4b1-54b461cda62a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        }
      },
      "source": [
        "set_manual_seed(42)\n",
        "\n",
        "mul_factor = 5\n",
        "num_epochs = 50\n",
        "\n",
        "data_loaders = OrderedDict()\n",
        "data_loaders['train'] = DataLoader(train_ds, batch_size=32, num_workers=8, pin_memory=True, drop_last=True,\n",
        "                                   sampler=WeightedRandomSampler(np.ones(len(train_ds)), len(train_ds) * mul_factor))\n",
        "data_loaders['valid'] = DataLoader(valid_ds, batch_size=32, num_workers=4, pin_memory=True, drop_last=False)\n",
        "\n",
        "model = LinkNet34(num_classes).cuda()\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', patience=5, factor=0.2)\n",
        "early_stopping = EarlyStoppingCallback(patience=20, metric='iou', minimize=False)\n",
        "\n",
        "# model training\n",
        "runner = SupervisedRunner()\n",
        "runner.train(\n",
        "    model=model,\n",
        "    criterion=nn.CrossEntropyLoss(),\n",
        "    optimizer=optimizer,\n",
        "    callbacks=[\n",
        "        iou_score, \n",
        "        early_stopping,\n",
        "        show_batches,\n",
        "        SchedulerCallback(reduce_metric='iou'),\n",
        "    ],\n",
        "    logdir='runs/linknet34/baseline',\n",
        "    loaders=data_loaders,\n",
        "    num_epochs=num_epochs,\n",
        "    scheduler=scheduler,\n",
        "    verbose=False,\n",
        "    main_metric='iou',\n",
        "    minimize_metric=False\n",
        ")\n",
        "\n",
        "# Cleanup after ourselves to free up GPU memory\n",
        "del scheduler, optimizer, runner, model, data_loaders "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using manual seed: 42\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/checkpoints/resnet34-333f7ec4.pth\n",
            "100%|██████████| 87306240/87306240 [00:01<00:00, 74680550.83it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2019-08-19 07:05:15,580] \n",
            "0/50 * Epoch 0 (train): _base/lr=0.0010 | _base/momentum=0.9000 | _timers/_fps=1128.2311 | _timers/batch_time=0.2502 | _timers/data_time=0.1322 | _timers/model_time=0.1179 | iou=0.1113 | iou_bicyclist=0.0002 | iou_building=0.2208 | iou_car=0.0003 | iou_fence=0.0015 | iou_pavement=0.000000E+00 | iou_pedestrian=0.000000E+00 | iou_pole=0.000000E+00 | iou_road=0.3692 | iou_signsymbol=0.000000E+00 | iou_sky=0.4618 | iou_tree=0.0136 | loss=1.4589\n",
            "0/50 * Epoch 0 (valid): _base/lr=0.0010 | _base/momentum=0.9000 | _timers/_fps=924.1296 | _timers/batch_time=2.4868 | _timers/data_time=0.4168 | _timers/model_time=2.0698 | iou=0.1830 | iou_bicyclist=0.000000E+00 | iou_building=0.4897 | iou_car=0.000000E+00 | iou_fence=0.000000E+00 | iou_pavement=0.000000E+00 | iou_pedestrian=0.000000E+00 | iou_pole=0.000000E+00 | iou_road=0.6213 | iou_signsymbol=0.000000E+00 | iou_sky=0.8866 | iou_tree=0.0055 | loss=1.2750\n",
            "[2019-08-19 07:07:05,258] \n",
            "1/50 * Epoch 1 (train): _base/lr=0.0010 | _base/momentum=0.9000 | _timers/_fps=1154.8598 | _timers/batch_time=0.1875 | _timers/data_time=0.1548 | _timers/model_time=0.0326 | iou=0.2853 | iou_bicyclist=0.000000E+00 | iou_building=0.6186 | iou_car=0.2291 | iou_fence=0.000000E+00 | iou_pavement=2.139642E-06 | iou_pedestrian=0.000000E+00 | iou_pole=0.000000E+00 | iou_road=0.8141 | iou_signsymbol=0.000000E+00 | iou_sky=0.8346 | iou_tree=0.2684 | loss=0.7143\n",
            "1/50 * Epoch 1 (valid): _base/lr=0.0010 | _base/momentum=0.9000 | _timers/_fps=1710.3364 | _timers/batch_time=0.4245 | _timers/data_time=0.4087 | _timers/model_time=0.0157 | iou=0.1861 | iou_bicyclist=0.000000E+00 | iou_building=0.3755 | iou_car=0.2478 | iou_fence=0.000000E+00 | iou_pavement=0.000000E+00 | iou_pedestrian=0.000000E+00 | iou_pole=0.000000E+00 | iou_road=0.4181 | iou_signsymbol=0.000000E+00 | iou_sky=0.8142 | iou_tree=0.1794 | loss=1.9779\n",
            "[2019-08-19 07:08:56,927] \n",
            "2/50 * Epoch 2 (train): _base/lr=0.0010 | _base/momentum=0.9000 | _timers/_fps=1139.2269 | _timers/batch_time=0.1498 | _timers/data_time=0.1185 | _timers/model_time=0.0313 | iou=0.3767 | iou_bicyclist=0.000000E+00 | iou_building=0.6180 | iou_car=0.4850 | iou_fence=0.000000E+00 | iou_pavement=0.2909 | iou_pedestrian=0.000000E+00 | iou_pole=0.000000E+00 | iou_road=0.9166 | iou_signsymbol=0.000000E+00 | iou_sky=0.8756 | iou_tree=0.5003 | loss=0.5646\n",
            "2/50 * Epoch 2 (valid): _base/lr=0.0010 | _base/momentum=0.9000 | _timers/_fps=1566.2397 | _timers/batch_time=0.3866 | _timers/data_time=0.3535 | _timers/model_time=0.0330 | iou=0.3946 | iou_bicyclist=0.000000E+00 | iou_building=0.7394 | iou_car=0.2563 | iou_fence=0.000000E+00 | iou_pavement=0.7499 | iou_pedestrian=0.000000E+00 | iou_pole=0.000000E+00 | iou_road=0.9361 | iou_signsymbol=0.000000E+00 | iou_sky=0.9357 | iou_tree=0.7083 | loss=0.5756\n",
            "[2019-08-19 07:10:48,579] \n",
            "3/50 * Epoch 3 (train): _base/lr=0.0010 | _base/momentum=0.9000 | _timers/_fps=1116.3086 | _timers/batch_time=0.1608 | _timers/data_time=0.1288 | _timers/model_time=0.0320 | iou=0.4246 | iou_bicyclist=0.000000E+00 | iou_building=0.6515 | iou_car=0.5942 | iou_fence=0.000000E+00 | iou_pavement=0.4811 | iou_pedestrian=0.000000E+00 | iou_pole=0.000000E+00 | iou_road=0.9386 | iou_signsymbol=0.000000E+00 | iou_sky=0.8910 | iou_tree=0.5948 | loss=0.4591\n",
            "3/50 * Epoch 3 (valid): _base/lr=0.0010 | _base/momentum=0.9000 | _timers/_fps=1670.5852 | _timers/batch_time=0.4063 | _timers/data_time=0.3812 | _timers/model_time=0.0250 | iou=0.4241 | iou_bicyclist=0.000000E+00 | iou_building=0.8156 | iou_car=0.3758 | iou_fence=0.000000E+00 | iou_pavement=0.7686 | iou_pedestrian=0.000000E+00 | iou_pole=0.000000E+00 | iou_road=0.9367 | iou_signsymbol=0.000000E+00 | iou_sky=0.9324 | iou_tree=0.8212 | loss=0.4245\n",
            "[2019-08-19 07:12:34,561] \n",
            "4/50 * Epoch 4 (train): _base/lr=0.0010 | _base/momentum=0.9000 | _timers/_fps=1129.9147 | _timers/batch_time=0.1392 | _timers/data_time=0.1064 | _timers/model_time=0.0327 | iou=0.4446 | iou_bicyclist=0.000000E+00 | iou_building=0.6902 | iou_car=0.6524 | iou_fence=0.000000E+00 | iou_pavement=0.5336 | iou_pedestrian=3.286266E-06 | iou_pole=0.000000E+00 | iou_road=0.9524 | iou_signsymbol=0.000000E+00 | iou_sky=0.8905 | iou_tree=0.6247 | loss=0.3997\n",
            "4/50 * Epoch 4 (valid): _base/lr=0.0010 | _base/momentum=0.9000 | _timers/_fps=1689.3345 | _timers/batch_time=0.3886 | _timers/data_time=0.3618 | _timers/model_time=0.0268 | iou=0.4210 | iou_bicyclist=0.000000E+00 | iou_building=0.7763 | iou_car=0.3267 | iou_fence=0.000000E+00 | iou_pavement=0.7533 | iou_pedestrian=0.000000E+00 | iou_pole=0.000000E+00 | iou_road=0.9439 | iou_signsymbol=0.000000E+00 | iou_sky=0.9320 | iou_tree=0.8705 | loss=0.4293\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMdCXYcwfKVz"
      },
      "source": [
        "Let's see what is the mIoU score on validation dataset for the best checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCarUtAGvvDM"
      },
      "source": [
        "baseline = load_checkpoint('runs/linknet34/baseline/checkpoints/best.pth')\n",
        "baseline_epoch = baseline['epoch']\n",
        "baseline_valid_metrics = baseline['valid_metrics']\n",
        "\n",
        "del baseline\n",
        "\n",
        "print('Baseline result mIoU:', baseline_valid_metrics['iou'], 'after', baseline_epoch, 'epochs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzHjj9K82Gk6"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdnMIjUtXZIh"
      },
      "source": [
        "## Class balancing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK8BYCEQXhCa"
      },
      "source": [
        "from sklearn.utils import compute_sample_weight\n",
        "\n",
        "def get_balanced_weights(dataset:CamVidDataset):\n",
        "    labels=[]\n",
        "    for mask in dataset.masks_fps:\n",
        "      mask = fs.read_image_as_is(mask)\n",
        "      unique_labels = np.unique(mask)\n",
        "      labels.append(''.join([str(int(i)) for i in unique_labels]))\n",
        "\n",
        "    weights = compute_sample_weight('balanced', labels)\n",
        "    return weights\n",
        "\n",
        "  \n",
        "set_manual_seed(43)\n",
        "\n",
        "mul_factor = 5\n",
        "num_epochs = 50\n",
        "\n",
        "train_sampler = WeightedRandomSampler(get_balanced_weights(train_ds), len(train_ds) * mul_factor)\n",
        "\n",
        "data_loaders = OrderedDict()\n",
        "data_loaders['train'] = DataLoader(train_ds, batch_size=32, num_workers=8, \n",
        "                                   pin_memory=True, \n",
        "                                   drop_last=True,\n",
        "                                   sampler=train_sampler)\n",
        "\n",
        "data_loaders['valid'] = DataLoader(valid_ds, batch_size=32, num_workers=4, \n",
        "                                   pin_memory=True,\n",
        "                                   drop_last=False)\n",
        "\n",
        "model = LinkNet34(num_classes).cuda()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Use baseline model as starting point\n",
        "restore_from_checkpoint('runs/linknet34/checkpoints/best.pth', model, optimizer)\n",
        "\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', patience=5, factor=0.2)\n",
        "early_stopping = EarlyStoppingCallback(patience=20, metric='iou', minimize=False)\n",
        "\n",
        "# model training\n",
        "runner = SupervisedRunner()\n",
        "runner.train(\n",
        "    model=model,\n",
        "    criterion=nn.CrossEntropyLoss(),\n",
        "    optimizer=optimizer,\n",
        "    callbacks=[\n",
        "        iou_score, \n",
        "        early_stopping,\n",
        "        show_batches,\n",
        "        SchedulerCallback(reduce_metric='iou'),\n",
        "    ],\n",
        "    logdir='runs/linknet34/baseline_balanced',\n",
        "    loaders=data_loaders,\n",
        "    num_epochs=num_epochs,\n",
        "    scheduler=scheduler,\n",
        "    verbose=False,\n",
        "    main_metric='iou',\n",
        "    minimize_metric=False\n",
        ")\n",
        "\n",
        "# Cleanup after ourselves to free up GPU memory\n",
        "del scheduler, optimizer, runner, model, data_loaders "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOeW3APmOFJO"
      },
      "source": [
        "## Training with CE + Lovazsh loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBwDMJm0oa1Y"
      },
      "source": [
        "set_manual_seed(42)\n",
        "\n",
        "model = LinkNet34(num_classes).cuda()\n",
        "\n",
        "baseline_checkpoint = load_checkpoint('runs/linknet34/baseline/checkpoints/best.pth')\n",
        "unpack_checkpoint(baseline_checkpoint, model=model)\n",
        "print('Loaded model weights from baseline')\n",
        "\n",
        "model = LinkNet34(num_classes).cuda()\n",
        "\n",
        "data_loaders = OrderedDict()\n",
        "data_loaders['train'] = DataLoader(train_ds, batch_size=32, num_workers=8, pin_memory=True, drop_last=True,\n",
        "                                   sampler=WeightedRandomSampler(np.ones(len(train_ds)), len(train_ds) * mul_factor))\n",
        "data_loaders['valid'] = DataLoader(valid_ds, batch_size=32, num_workers=4, pin_memory=True, drop_last=False)\n",
        "\n",
        "criterion = JointLoss(nn.CrossEntropyLoss(), LovashLoss(classes=np.arange(11)), 1.0, 0.5)\n",
        "optimizer = Adam(model.parameters(), lr=1e-4)\n",
        "scheduler = ReduceLROnPlateau(optimizer, patience=5)\n",
        "early_stopping = EarlyStoppingCallback(patience=12, metric='iou', minimize=False)\n",
        "\n",
        "# model training\n",
        "runner = SupervisedRunner()\n",
        "runner.train(\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    callbacks=[iou_score, early_stopping],\n",
        "    logdir='runs/linknet34/bce_dice',\n",
        "    loaders=data_loaders,\n",
        "    num_epochs=num_epochs,\n",
        "    scheduler=scheduler,\n",
        "    verbose=False,\n",
        "    main_metric='iou',\n",
        "    minimize_metric=False\n",
        ")\n",
        "\n",
        "# Cleanup after ourselves to free up GPU memory\n",
        "del scheduler, optimizer, runner, model, data_loaders "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CZQF7CwCh2V"
      },
      "source": [
        "set_manual_seed(42)\n",
        "\n",
        "model = LinkNet34(num_classes).cuda()\n",
        "\n",
        "baseline_checkpoint = load_checkpoint('runs/linknet34/baseline/checkpoints/best.pth')\n",
        "unpack_checkpoint(baseline_checkpoint, model=model)\n",
        "print('Loaded model weights from baseline')\n",
        "\n",
        "model = LinkNet34(num_classes).cuda()\n",
        "\n",
        "data_loaders = OrderedDict()\n",
        "data_loaders['train'] = DataLoader(train_ds, batch_size=32, num_workers=8, pin_memory=True, drop_last=True,\n",
        "                                   sampler=WeightedRandomSampler(np.ones(len(train_ds)), len(train_ds) * mul_factor))\n",
        "data_loaders['valid'] = DataLoader(valid_ds, batch_size=32, num_workers=4, pin_memory=True, drop_last=False)\n",
        "\n",
        "criterion = JointLoss(nn.CrossEntropyLoss(), MulticlassJaccardLoss(classes=np.arange(11)), 1.0, 0.5)\n",
        "optimizer = Adam(model.parameters(), lr=1e-4)\n",
        "scheduler = ReduceLROnPlateau(optimizer, patience=5)\n",
        "early_stopping = EarlyStoppingCallback(patience=12, metric='iou', minimize=False)\n",
        "\n",
        "# model training\n",
        "runner = SupervisedRunner()\n",
        "runner.train(\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    callbacks=[iou_score, early_stopping],\n",
        "    logdir='runs/linknet34/bce_jaccard',\n",
        "    loaders=data_loaders,\n",
        "    num_epochs=num_epochs,\n",
        "    scheduler=scheduler,\n",
        "    verbose=False,\n",
        "    main_metric='iou',\n",
        "    minimize_metric=False\n",
        ")\n",
        "\n",
        "# Cleanup after ourselves to free up GPU memory\n",
        "del scheduler, optimizer, runner, model, data_loaders "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WADP2keZqep"
      },
      "source": [
        "del scheduler, optimizer, runner, model, data_loaders "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSlYf0AFi9og"
      },
      "source": [
        "# Fine-tuning model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjqjoWzIqT2u"
      },
      "source": [
        "## Fine-tuning decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlrI6E93qV1M"
      },
      "source": [
        "from pytorch_toolbelt.utils.torch_utils import set_trainable\n",
        "from pytorch_toolbelt.utils.torch_utils import maybe_cuda, count_parameters\n",
        "\n",
        "set_manual_seed(42)\n",
        "\n",
        "mul_factor = 5\n",
        "num_epochs = 50\n",
        "\n",
        "model = LinkNet34(num_classes).cuda()\n",
        "\n",
        "# Freeze encoder\n",
        "set_trainable(model.firstconv, trainable=False, freeze_bn=True)\n",
        "set_trainable(model.firstbn, trainable=False, freeze_bn=True)\n",
        "set_trainable(model.encoder1, trainable=False, freeze_bn=True)\n",
        "set_trainable(model.encoder2, trainable=False, freeze_bn=True)\n",
        "set_trainable(model.encoder3, trainable=False, freeze_bn=True)\n",
        "set_trainable(model.encoder4, trainable=False, freeze_bn=True)\n",
        "\n",
        "baseline_checkpoint = load_checkpoint('runs/linknet34/baseline/checkpoints/best.pth')\n",
        "unpack_checkpoint(baseline_checkpoint, model=model)\n",
        "print('Loaded model weights from baseline')\n",
        "\n",
        "print(count_parameters(model))\n",
        "\n",
        "train_ds = CamVidDataset(x_train_dir, y_train_dir, transform=get_training_augmentation())\n",
        "valid_ds = CamVidDataset(x_valid_dir, y_valid_dir, transform=get_validation_augmentation())\n",
        "\n",
        "data_loaders = OrderedDict()\n",
        "data_loaders['train'] = DataLoader(train_ds, batch_size=32, num_workers=8, pin_memory=True, drop_last=True,\n",
        "                                   sampler=WeightedRandomSampler(np.ones(len(train_ds)), len(train_ds) * mul_factor))\n",
        "data_loaders['valid'] = DataLoader(valid_ds, batch_size=32, num_workers=4, pin_memory=True, drop_last=False)\n",
        "\n",
        "optimizer = SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5, momentum=0.9, weight_decay=1e-4)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', patience=5, factor=0.2)\n",
        "early_stopping = EarlyStoppingCallback(patience=12, metric='iou', minimize=False)\n",
        "\n",
        "# model training\n",
        "runner = SupervisedRunner()\n",
        "runner.train(\n",
        "    model=model,\n",
        "    criterion=nn.CrossEntropyLoss(),\n",
        "    optimizer=optimizer,\n",
        "    callbacks=[\n",
        "        iou_score, \n",
        "        early_stopping,\n",
        "        show_batches,\n",
        "        SchedulerCallback(reduce_metric='iou'),\n",
        "    ],\n",
        "    logdir='runs/linknet34/finetune',\n",
        "    loaders=data_loaders,\n",
        "    num_epochs=num_epochs,\n",
        "    scheduler=scheduler,\n",
        "    verbose=False,\n",
        "    main_metric='iou',\n",
        "    minimize_metric=False\n",
        ")\n",
        "\n",
        "# Cleanup after ourselves to free up GPU memory\n",
        "del scheduler, optimizer, runner, model, data_loaders  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A73pg60Qp_cO"
      },
      "source": [
        "## Fine-tuning with FocalLoss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRwv6x1FqBgg"
      },
      "source": [
        "from catalyst.utils import unpack_checkpoint, load_checkpoint\n",
        "from pytorch_toolbelt.losses.functional import sigmoid_focal_loss, reduced_focal_loss\n",
        "from torch.nn.modules.loss import _Loss\n",
        "\n",
        "set_manual_seed(42)\n",
        "\n",
        "class FocalLoss(_Loss):\n",
        "    def __init__(self, alpha=0.5, gamma=2, ignore=None):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.ignore = ignore\n",
        "\n",
        "    def forward(self, label_input, label_target):\n",
        "        \"\"\"Compute focal loss for multi-class problem.\n",
        "        Ignores anchors having -1 target label\n",
        "        \"\"\"\n",
        "        num_classes = label_input.size(1)\n",
        "        loss = 0\n",
        "\n",
        "        # Filter anchors with -1 label from loss computation\n",
        "        if self.ignore is not None:\n",
        "            not_ignored = label_target != self.ignore\n",
        "            \n",
        "        for cls in range(num_classes):\n",
        "            cls_label_target = (label_target == cls).long()\n",
        "            cls_label_input = label_input[:, cls, ...]\n",
        "\n",
        "            if self.ignore is not None:\n",
        "                cls_label_target = cls_label_target[not_ignored]\n",
        "                cls_label_input = cls_label_input[not_ignored]\n",
        "\n",
        "            loss += sigmoid_focal_loss(cls_label_input, cls_label_target, gamma=self.gamma, alpha=self.alpha)\n",
        "        return loss\n",
        "\n",
        "      \n",
        "mul_factor = 5\n",
        "num_epochs = 50\n",
        "\n",
        "data_loaders = OrderedDict()\n",
        "data_loaders['train'] = DataLoader(train_ds, batch_size=32, num_workers=8, pin_memory=True, drop_last=True,\n",
        "                                   sampler=WeightedRandomSampler(np.ones(len(train_ds)), len(train_ds) * mul_factor))\n",
        "data_loaders['valid'] = DataLoader(valid_ds, batch_size=32, num_workers=4, pin_memory=True, drop_last=False)\n",
        "\n",
        "num_classes = len(CLASS_NAMES)\n",
        "model = LinkNet34(num_classes).cuda()\n",
        "\n",
        "try:\n",
        "  baseline_checkpoint = load_checkpoint('runs/linknet34/finetune/checkpoints/best.pth')\n",
        "  unpack_checkpoint(baseline_checkpoint, model=model)\n",
        "  print('Loaded model weights from finetune')\n",
        "except:\n",
        "  print('Failed to load previous state. Training from scratch')\n",
        "  \n",
        "# model runner\n",
        "runner = SupervisedRunner()\n",
        "\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10,20,30,40], gamma=0.5)\n",
        "\n",
        "# model training\n",
        "runner = SupervisedRunner()\n",
        "runner.train(\n",
        "    model=model,\n",
        "    criterion=FocalLoss(alpha=None),\n",
        "    optimizer=optimizer,\n",
        "    callbacks=[\n",
        "        iou_score, \n",
        "        early_stopping,\n",
        "        show_batches\n",
        "    ],\n",
        "    logdir='runs/linknet34/focal',\n",
        "    loaders=data_loaders,\n",
        "    num_epochs=num_epochs,\n",
        "    scheduler=scheduler,\n",
        "    verbose=False,\n",
        "    main_metric='iou',\n",
        "    minimize_metric=False\n",
        ")\n",
        "\n",
        "# Cleanup after ourselves to free up GPU memory\n",
        "del scheduler, optimizer, runner, model, data_loaders  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmE7WJ7Zbia4"
      },
      "source": [
        "## Train and test with TTA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkXdhIf4btUk"
      },
      "source": [
        "from pytorch_toolbelt.inference import tta\n",
        "      \n",
        "set_manual_seed(42)\n",
        "\n",
        "mul_factor = 5\n",
        "num_epochs = 50\n",
        "\n",
        "model = LinkNet34(num_classes).cuda()\n",
        "\n",
        "try:\n",
        "  baseline_checkpoint = load_checkpoint('runs/linknet34/baseline/checkpoints/best.pth')\n",
        "  unpack_checkpoint(baseline_checkpoint, model=model)\n",
        "  print('Loaded model weights from baseline')\n",
        "except:\n",
        "  print('Failed to load previous state. Training from scratch')\n",
        "\n",
        "model = tta.MultiscaleTTAWrapper(model, [0.8, 1.0, 1.5])\n",
        "# model = tta.TTAWrapper(model, tta.fliplr_image2mask)\n",
        "\n",
        "print(count_parameters(model))\n",
        "\n",
        "train_ds = CamVidDataset(x_train_dir, y_train_dir, transform=get_training_augmentation())\n",
        "valid_ds = CamVidDataset(x_valid_dir, y_valid_dir, transform=get_validation_augmentation())\n",
        "\n",
        "data_loaders = OrderedDict()\n",
        "data_loaders['train'] = DataLoader(train_ds, batch_size=8, num_workers=8, pin_memory=True, drop_last=True,\n",
        "                                   sampler=WeightedRandomSampler(np.ones(len(train_ds)), len(train_ds) * mul_factor))\n",
        "data_loaders['valid'] = DataLoader(valid_ds, batch_size=8, num_workers=4, pin_memory=True, drop_last=False)\n",
        "\n",
        "optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
        "scheduler = ReduceLROnPlateau(optimizer, patience=5)\n",
        "early_stopping = EarlyStoppingCallback(patience=12, metric='iou', minimize=False)\n",
        "\n",
        "# model training\n",
        "runner = SupervisedRunner()\n",
        "runner.train(\n",
        "    model=model,\n",
        "    criterion=nn.CrossEntropyLoss(),\n",
        "    optimizer=optimizer,\n",
        "    callbacks=[iou_score, early_stopping],\n",
        "    logdir='runs/linknet34/tta',\n",
        "    loaders=data_loaders,\n",
        "    num_epochs=num_epochs,\n",
        "    scheduler=scheduler,\n",
        "    verbose=False,\n",
        "    main_metric='iou',\n",
        "    minimize_metric=False\n",
        ")\n",
        "\n",
        "# Cleanup after ourselves to free up GPU memory\n",
        "del scheduler, optimizer, runner, model, data_loaders "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NPIlgoNl6wI"
      },
      "source": [
        "baseline = load_checkpoint('runs/linknet34/baseline/checkpoints/tta.pth')\n",
        "baseline_epoch = baseline['epoch']\n",
        "baseline_valid_metrics = baseline['valid_metrics']\n",
        "\n",
        "del baseline\n",
        "\n",
        "print('TTA result mIoU:', baseline_valid_metrics['iou'],'after',baseline_epoch,'epochs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oUvW8U3gc0O"
      },
      "source": [
        "## Train with deep supervision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgVBwCXqifiq"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUYGCChBnqyk"
      },
      "source": [
        "# Cleanup after ourselves to free up GPU memory\n",
        "try:\n",
        "  del scheduler\n",
        "except:\n",
        "  pass\n",
        "\n",
        "try:\n",
        "  del optimizer\n",
        "except:\n",
        "  pass\n",
        "\n",
        "try:\n",
        "  del runner\n",
        "except:\n",
        "  pass\n",
        "\n",
        "try:\n",
        "  del model\n",
        "except:\n",
        "  pass\n",
        "\n",
        "try:\n",
        "  del data_loaders\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWx7tCV-vJtk"
      },
      "source": [
        "class DecoderBlockLinkNet(nn.Module):\n",
        "    def __init__(self, in_channels, n_filters):\n",
        "        super().__init__()\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # B, C, H, W -> B, C/4, H, W\n",
        "        self.conv1 = nn.Conv2d(in_channels, in_channels // 4, 1)\n",
        "        self.norm1 = nn.BatchNorm2d(in_channels // 4)\n",
        "\n",
        "        # B, C/4, H, W -> B, C/4, 2 * H, 2 * W\n",
        "        self.deconv2 = nn.ConvTranspose2d(in_channels // 4, in_channels // 4, kernel_size=4,\n",
        "                                          stride=2, padding=1, output_padding=0)\n",
        "        self.norm2 = nn.BatchNorm2d(in_channels // 4)\n",
        "\n",
        "        # B, C/4, H, W -> B, C, H, W\n",
        "        self.conv3 = nn.Conv2d(in_channels // 4, n_filters, 1)\n",
        "        self.norm3 = nn.BatchNorm2d(n_filters)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.norm1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.deconv2(x)\n",
        "        x = self.norm2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.norm3(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "      \n",
        "class LinkNet34(nn.Module):\n",
        "    def __init__(self, num_classes=1, num_channels=3, pretrained=True):\n",
        "        super().__init__()\n",
        "        assert num_channels == 3\n",
        "        self.num_classes = num_classes\n",
        "        filters = [64, 128, 256, 512]\n",
        "        resnet = resnet34(pretrained=pretrained)\n",
        "\n",
        "        self.firstconv = resnet.conv1\n",
        "        self.firstbn = resnet.bn1\n",
        "        self.firstrelu = resnet.relu\n",
        "        self.firstmaxpool = resnet.maxpool\n",
        "        self.encoder1 = resnet.layer1\n",
        "        self.encoder2 = resnet.layer2\n",
        "        self.encoder3 = resnet.layer3\n",
        "        self.encoder4 = resnet.layer4\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder4 = DecoderBlockLinkNet(filters[3], filters[2])\n",
        "        self.decoder3 = DecoderBlockLinkNet(filters[2], filters[1])\n",
        "        self.decoder2 = DecoderBlockLinkNet(filters[1], filters[0])\n",
        "        self.decoder1 = DecoderBlockLinkNet(filters[0], filters[0])\n",
        "      \n",
        "        self.center_logits = nn.Conv2d(filters[3], num_classes, kernel_size=1)\n",
        "        self.coarse_logits = nn.Conv2d(filters[0], num_classes, kernel_size=1)\n",
        "        \n",
        "        # Final Classifier\n",
        "        self.finaldeconv1 = nn.ConvTranspose2d(filters[0], 32, 3, stride=2)\n",
        "        self.finalrelu1 = nn.ReLU(inplace=True)\n",
        "        self.finalconv2 = nn.Conv2d(32, 32, 3)\n",
        "        self.finalrelu2 = nn.ReLU(inplace=True)\n",
        "        self.finalconv3 = nn.Conv2d(32, num_classes, 2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "      \n",
        "        # Encoder\n",
        "        x = self.firstconv(x)\n",
        "        x = self.firstbn(x)\n",
        "        x = self.firstrelu(x)\n",
        "        x = self.firstmaxpool(x)\n",
        "        e1 = self.encoder1(x)\n",
        "        e2 = self.encoder2(e1)\n",
        "        e3 = self.encoder3(e2)\n",
        "        e4 = self.encoder4(e3)\n",
        "\n",
        "        # Decoder with Skip Connections\n",
        "        d4 = self.decoder4(e4) + e3\n",
        "        d3 = self.decoder3(d4) + e2\n",
        "        d2 = self.decoder2(d3) + e1\n",
        "        d1 = self.decoder1(d2)\n",
        "\n",
        "        # Final Classification\n",
        "        f1 = self.finaldeconv1(d1)\n",
        "        f2 = self.finalrelu1(f1)\n",
        "        f3 = self.finalconv2(f2)\n",
        "        f4 = self.finalrelu2(f3)\n",
        "        f5 = self.finalconv3(f4)\n",
        "                \n",
        "        return {\n",
        "          'logits': f5,\n",
        "          'center_logits': self.center_logits(e4),\n",
        "          'coarse_logits': self.coarse_logits(d1)\n",
        "        }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_ueC377gfbk"
      },
      "source": [
        "from catalyst.dl import CriterionCallback\n",
        "import cv2\n",
        "\n",
        "class DSVCamVidDataset(CamVidDataset):\n",
        "  \n",
        "    def __getitem__(self, i):\n",
        "        image = fs.read_rgb_image(self.images_fps[i])\n",
        "        mask = fs.read_image_as_is(self.masks_fps[i])\n",
        "        assert mask.max() < len(CLASS_NAMES)\n",
        "\n",
        "        # apply augmentations\n",
        "        sample = self.transform(image=image, mask=mask)\n",
        "        image, mask = sample['image'], sample['mask']\n",
        "        \n",
        "        h, w = mask.shape[:2]\n",
        "        \n",
        "        # Coarse targets has stride of 2\n",
        "        coarse_targets = cv2.resize(mask, (w // 2, h // 2), interpolation=cv2.INTER_NEAREST)\n",
        "        center_targets = cv2.resize(coarse_targets, (w // 32, h // 32), interpolation=cv2.INTER_NEAREST)\n",
        "        \n",
        "        return {\n",
        "            \"image_id\": id_from_fname(self.images_fps[i]),\n",
        "            \"features\": tensor_from_rgb_image(image),\n",
        "            \"targets\": torch.from_numpy(mask).long(),\n",
        "            \"coarse_targets\": torch.from_numpy(coarse_targets).long(),\n",
        "            \"center_targets\": torch.from_numpy(center_targets).long(),\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "set_manual_seed(42)\n",
        "\n",
        "mul_factor = 5\n",
        "num_epochs = 150\n",
        "\n",
        "train_ds = DSVCamVidDataset(x_train_dir, y_train_dir, transform=get_training_augmentation())\n",
        "valid_ds = DSVCamVidDataset(x_valid_dir, y_valid_dir, transform=get_validation_augmentation())\n",
        "\n",
        "sample = train_ds[0]\n",
        "print(sample['targets'].size(), sample['coarse_targets'].size(),  sample['center_targets'].size())\n",
        "\n",
        "data_loaders = OrderedDict()\n",
        "data_loaders['train'] = DataLoader(train_ds, batch_size=16, num_workers=8, pin_memory=True, drop_last=True,\n",
        "                                   sampler=WeightedRandomSampler(np.ones(len(train_ds)), len(train_ds) * mul_factor))\n",
        "data_loaders['valid'] = DataLoader(valid_ds, batch_size=16, num_workers=4, pin_memory=True, drop_last=False)\n",
        "\n",
        "model = LinkNet34(num_classes).cuda()\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "scheduler = ReduceLROnPlateau(optimizer, patience=5)\n",
        "early_stopping = EarlyStoppingCallback(patience=12, metric='iou', minimize=False)\n",
        "\n",
        "# model training\n",
        "runner = SupervisedRunner()\n",
        "runner.train(\n",
        "    model=model,\n",
        "    criterion=nn.CrossEntropyLoss(),\n",
        "    optimizer=optimizer,\n",
        "    callbacks=[\n",
        "      CriterionCallback(input_key='targets', output_key='logits', prefix='loss'),\n",
        "      CriterionCallback(input_key='coarse_targets', output_key='coarse_logits', prefix='coarse_loss', multiplier=0.5),\n",
        "      CriterionCallback(input_key='center_targets', output_key='center_logits', prefix='center_loss', multiplier=0.25),\n",
        "      iou_score, \n",
        "      early_stopping\n",
        "    ],\n",
        "    logdir='runs/linknet34/dsv',\n",
        "    loaders=data_loaders,\n",
        "    num_epochs=num_epochs,\n",
        "    scheduler=scheduler,\n",
        "    verbose=False,\n",
        "    main_metric='iou',\n",
        "    minimize_metric=False\n",
        ")\n",
        "\n",
        "# Cleanup after ourselves to free up GPU memory\n",
        "del scheduler, optimizer, runner, model, data_loaders"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmVpoyD3juY2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}